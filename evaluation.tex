%!TEX root = main.tex

\chapter{Evaluation}
In this chapter we conclude our work by looking at the goals defined in the introduction, and evaluate the results.
In Section \ref{sec:evalimp} we evaluate our implementation.
In Section \ref{sec:evalperf} we evaluate the in-game performance of our implementation.
Section \ref{sec:conclusion} contains the final conclusion for the project and this report.
Section \ref{sec:futurework} contains our thoughts on further improvements that can be made on our project.

\section{Evaluation of Implementation}
\label{sec:evalimp}
Our motivation for this project was to create an agent based on the LIDA model, that could play StarCraft: Brood War. we did this by implementing a simple proof of concept agent using the LIDA framework. We also discovered some of the problems inherent in the use of this cognitive architecture for this domain.

During the integration between LIDA and StarCraft we ran into a few problems. The most important one being that measurement of time in both processes are different. LIDA operates with ticks, where the length of a tick is dynamic, and is decided by the slowest LIDA process that is run in that specific tick. We had to sync these ticks up with the StarCraft measurement  of time which is frames. Because we have to pause the StarCraft game for each frame until the LIDA model returns from that tick this will probably disqualify the agent from participating in certain tournaments.

We discovered that because StarCraft is such a complex game with so many different possible states and situations the amount of feature detectors you have make a very significant impact on the performance of the agent. We only implemented some basic feature detectors to give the agent enough information about the environment to perform some simple tasks that made for some competition for the built-in AI in the game. But we constantly observed the agent getting into situations that for us as human observers was very clear, but the agent didn't see them. One such situation that happened repeatedly was that the game AI would attack us at the same time as we moved out to attack them. And because our agent wasn't capable of detecting this, it would never turned around to defend his own base against this attack.

But for every situation that we observed we could not deal with, there was obvious that simply more detectors would be able to parse this data and understand the situation. We were unable to see anything that would be outside the scope of reasoning with and handling in our architecture, if it had the right set of information to work with. Expanding upon the set of feature detectors would probably be one of the main priorities for improving the performance of the agent.

As mentioned in Section \ref{sec:action_selection} the Action Selection module in the current implementation of the LIDA framework is not fully implemented according to the definition in the LIDA model. Because of this the action selection in our agent is very limited in comparison to what it could be if the model was fully realized. In our agent the action selection module simply selects the behavior that has the highest activation when it gets instantiated in the Procedural Memory. This turned out to be one of the biggest limitations we discovered during implementation because we didn't have the option for one behavior to inhibit other behaviors. An example of this is the behavior that creates a new gateway, we didn't want to keep building new gateways after we made the first one. But because we were unable to inhibit this behavior the agent kept creating new gateways. There wasn't a good solution for this problem other then moving some of this logic to the feature detectors, and that is why we created the build-order feature detectors. Ideally you would probable want this logic in the action selection module.

LIDA framework version 1.2b released May, 2012 is the first version of the framework to include the Behavior Network that the LIDA model describes for the Action Selection module. We did not have time to include this update in our agent. The Behavior Networks adds a lot of utility to the Action Selection module. In essence it takes the instantiated behaviors from the Procedural Memory as input to a network of behaviors that can effect each other. Each activated behavior can impact other behaviors in the network by acting as amplifiers or inhibitors to connected behaviors. The behavior with the highest activation in the end is then selected as the action to be executed by the sensory-motor memory. With the updates LIDA framework the agent should be able to do much more complex reasoning in the action selection stage, and one could remove this logic from the feature detectors where it do not really belong.

Another important aspect that the LIDA framework is currently missing that is defined in the model description is learning. When the consciousness broadcast occurs several of the LIDA modules are suppose to receive the broadcast and use this for leaning. PAM is suppose to use this to learn different base activation values, nodes that have proven to be useful will have a grater change of being activated again. Also the Transient Episodic Memory should store the current focus of consciousness as short-term memory, and there is a chance for this to be moved to Declarative Memory for long term storage. These are just some of the modules that are suppose to learn during execution, a full list of the model defined learning types are: Procedural learning, Perceptual learning, Episodic learning and Attention learning. But none of these are currently supported by the framework, but they will be in in a future release.

The work we have done with integrating and  implementation for our agent should simplify the work of anyone else intending to do something similar. Our domain-specific classes should provide everything needed for any kind of agent using the LIDA framework for StarCraft: Brood War, and our implemented feature detectors and behavior should be a good starting point, as well as a good example for other feature detectors and schemes one needs to implement to create more complex behaviors for the agent.

\section{Evaluation of Agent Performance}
\label{sec:evalperf}
From the performance tests we ended up with a win ratio of 35\%. That is not to high, but it was also never our goal to create the perfect agent. Each time we lost, it was usually because the agent ended up in a situation where it didn't have the correct detectors and behaviors to handle that situation. This should mean that it is only a matter of extending on the implementation to improve the performance of the bot. The underlaying method seems to be working just fine when the environment is simple enough for this implementation to handle, so it should scale well with more complex detectors and schemes.

The problems we encountered that made us lose games was for instance that our agent does not detect the presence of enemy units attacking us, and we therefore rely on the built-in auto attack of enemy units for protection of our base. This often leads us to ``base-trades'', where we are able to destroy the enemy base, but are unable to protect our own.

We lack any kind of advanced micro behavior, which means we utilize the units produced very badly. This means that early aggression was very ineffective because they requite good micro of a few units, so we ended up removing that behavior from the agent. We also had big problems when the enemy created base defenses, like Protoss canons and Zerg spine crawlers. One observed behavior a long line of zealots simply marching into the line of fire of a canon, so they were cut down one by one. A more effective strategy here would be to group up the units, so some of them would be able to reach the canon and possibly destroy it.

Another issue is that our bot only builds one particular unit. This makes it very easy for a human to counter, for example with air units, which the zealots are unable to attack. When the behavior net is implemented in the agent it should be a lot easier to create more complex multi unit-type army compositions.
Our agents also never expands to additional bases. While this made things easier to implement (positioning of units etc. became easier to work with), it means that if we don't win quickly, we will run out of minerals, and we will be at a severe economic disadvantage if and when the opponent expands to several bases.

\section{Conclusion}
\label{sec:conclusion}
Our goals for his project was to integrate a cognitive model with BWAPI so we could use this as a base for building a proof of concept artificial intelligence agent that played StarCraft using cognitive methodologies.

We succeeded in creating this in the form of the Java agent Jantu that runs the LIDA framework on top of Java bindings for BWAPI. We discovered several limitation with the method, most of whom has to do with missing features in the framework that will be implemented in future releases. And while the in-game performance was not excellent, it is a very good starting point for any future work in this area.

We discovered that the domain of StarCraft is big, and that there are a lot of different situations that an AI agent will have to handle if it is going to perform well over time. For a cognitive implementation this means that it will require a lot of different feature detectors and schemes to handle every situation it might encounter. It is not given, however, that it will scale well when you start adding a lot of detectors and schemes, and it might become hard to keep track of the different states that will occur in Action Selection as the complexity grows.

\section{Future Work}
\label{sec:futurework}
There are several areas of this project that can be improved upon. Implementing more feature detectors, and more behaviours, might be the easiest and obvious way to go forward. Some core missing behaviors are for example grouping of units and managing the individual units better, expanding to extra bases for additional income, as well as more complex build orders and attack patterns.

A main priority should be to update the LIDA framework to the newest version and replacing the Action Selection Module with the more advanced Behavior network that was implemented in version 1.2 of the LIDA framework. This will be a significant change that will give more power to the decision making process of the agent, and make it more capable of complex reasoning.

Another significant improvement would be to use the various memory modules available more actively.\cite{franklin2007lida}. This should help increase the complexity of the current situational model by letting the bot remember more about previous states of the game. But in order for the memory modules to function properly, learning has to be first implemented into the framework according to the LIDA model specifications. The addition of learning will also help on modules then just episodic memory. When learning of base-level activation is implemented in the LIDA framework this can be used for improving the agents performance, by learning what build orders are good, and learning to prioritize e. g. military strength vs. economic stability.
