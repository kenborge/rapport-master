%!TEX root = main.tex

\chapter{Evaluation}
In this chapter we conclude our work by looking at the goals defined in the introduction, and evaluate the results.
In Section \ref{sec:evalperf} we evaluate the in-game performance of our implementation.
Section \ref{sec:evalimp} we evaluate our implementation.
Section \ref{sec:conclusion} contains the final conclusion for the project and this report.
Section \ref{sec:futurework} contains our thoughts on future implementation and improvements of our solution.


\section{Evaluation of Agent Performance}
\label{sec:evalperf}
Our agent performs rather badly, mostly because of a lack of feature detectors and behaviours. For instance, our agent does not detect the presence of enemy units at all, and we therefore rely on the built-in auto attack of enemy units for protection of our base. This often leads to ``base-trades'', where we are able to destroy the enemy base, but are unable to protect our own.

We also lack any kind of advanced microing behaviour, which means we utilize the units produced very badly. One observed behaviour was simply marching a long line of zealots into the line of fire of a canon, so they were cut down one by one. A more effective strategy here would be to group them up beforehand, so at least some of them would be able to reach the canon and possibly kill it.

Another issue is that our bot only builds one particular unit. This makes it very easy for a human to counter, for example with air units, which the zealots are unable to attack.

Another pretty grave flaw in our agents behaviour is that it never expands to several bases. While this made things easier to implement (positioning of units etc. became trivial), it means that if we don't win quickly, we will run out of minerals, and we will be at a severe economic disadvantage if/when the opponent expands to several bases.

But while the actual performance of the agent is not at a high-level, we can see that it is already working in a human-like fashion, switching conscious attention between the things that need to be done, and acting on it.

\section{Evaluation of Implementation}
\label{sec:evalimp}
Our motivation for this project was to create an agent based on the LIDA model, that could play StarCraft: Brood War. we did this by implementing a simple proof of concept agent using the LIDA framework. We also discovered some of the problems inherent in the use of this cognitive architecture for this domain.

During the integration between LIDA and StarCraft we ran into a few problems. The most important one being that measurement of time in both processes are different. LIDA operates with ticks, where the length of a tick is dynamic, and is decided by the slowest LIDA process that is run in that specific tick. We had to sync these ticks up with the StarCraft measurement  of time which is frames. Because we have to pause the StarCraft game for each frame until the LIDA model returns from that tick this will probably disqualify the agent from participating in certain tournaments.

We discovered that because StarCraft is such a complex game with so many different possible states and situations the amount of feature detectors you have make a very significant impact on the performance of the agent. We only implemented some basic feature detectors to give the agent enough information about the environment to perform some simple tasks that made for some competition for the built-in AI in the game. But we constantly observed the agent getting into situations that for us as human observers was very clear, but the agent didn't see them. Examples of this was, base trading where the game AI would attack us at the same time as we moved out to attack them, but we didn't detect this so our agent never turned around to defend against this attack.

But for every situation that we observed we could not deal with, there was obvious that simply more detectors would be able to parse this data and understand the situation. We were unable to see anything that would be outside the scope of reasoning with and handling in our architecture, if it had the right set of information to work with. Expanding upon the set of feature detectors would probably be one of the main priorities for improving the performance of the agent.

As mentioned in section \ref{sec:action_selection} the Action Selection module in the current implementation of the LIDA framework is not fully implemented according to the definition in the LIDA model. Because of this the action selection in our agent is very limited in comparison to what it could be if the model was fully realized. In our agent the action selection module simply selects the behaviour that has the highest activation when it gets instantiated in the Procedural Memory. This turned out to be one of the biggest limitations we discovered during implementation because we didn't have the option for one behavior to inhibit other behaviors. An example of this is the behavior that creates a new Gateway, we didn't want to keep building new gateways after we made the first one. But because we were unable to inhibit this behavior the agent kept creating new gateways. There wasn't a good solution for this problem other then moving some of this logic to the feature detectors, and that is why we created the build-order feature detectors. Ideally you would probable want this logic in the action selection module.

LIDA framework version 1.2b released May, 2012 is the first version of the framework to include the Behavior Network that the LIDA model describes for the Action Selection module. We did not have time to include this update in our agent. The Behavior Networks adds a lot of utility to the Action Selection module. In essence it takes the instantiated behaviors from the Procedural Memory as input to a network of behaviors that can effect each other. Each activated behavior can impact other behaviors in the network by acting as amplifiers or inhibitors to connected behaviors. The behavior with the highest activation in the end is then selected as the action to be executed by the sensory-motor memory. With the updates LIDA framework the agent should be able to do much more complex reasoning in the action selection stage, and one could remove this logic from the feature detectors where it do not really belong.

We believe that the work we have done integrating LIDA and BWAPI, and implementing our agent, should simplify the work of anyone else intending to do something similar. Our domain-specific classes should provide everything needed for any kind of agent using the LIDA framework for StarCraft: Brood War, and our implemented feature detectors and behaviour should be a good starting point, as well as a good example for other feature detectors and schemes one needs to implement to create more complex behaviors for the agent.

\section{Conclusion}
\label{sec:conclusion}
We succeeded in achieving what we set out to do, and while the in-game performance was not excellent, we think it is a very good starting point for any future work in this area.

\section{Future Work}
\label{sec:futurework}
We believe that there are several areas to be worked on. Implementing more feature detectors, and more behaviours, might be the easiest and obvious way to go forward. Obviously missing behaviour is for example grouping and managing the individual units better, expanding to extra bases for additional income, as well as more complex build orders, instead of the extremely simple timing attack implemented in our bot.

Replacing the rule-based action selection module with a more advanced approach is also something to look into. Version 1.2 of the LIDA framework replaces the default rule-based action selection with a behaviour network, based on Maes' behaviour network.\cite{maes1989right}

A more significant change would be to use the various memory modules available more actively.\cite{franklin2007lida}. This could help with for example letting the bot remember more about the state of the game, for example if it has already built the necessary building necessary for a certain unit, or remember build orders (learned from earlier games).

Learning could also be implemented at several other levels, not just as long-term memory.
